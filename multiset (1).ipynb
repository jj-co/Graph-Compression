{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiset",
      "provenance": [],
      "collapsed_sections": [
        "sZmYNTDBnG93",
        "KeY1ene0aJbj",
        "tDSf_ziwbPhZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AZT2L26NyJF"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
        "plt.rcParams['font.family'] = \"serif\"\n",
        "df = pd.pivot_table(data=sns.load_dataset(\"flights\"),\n",
        "                    index='month',\n",
        "                    values='passengers',\n",
        "                    columns='year')\n",
        "print(df.head())\n",
        "p = sns.heatmap(df, cmap='coolwarm', annot=True, fmt=\".1f\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCYGOoJHdNEc"
      },
      "source": [
        "sns.load_dataset(\"flights\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtHxDR0YPG_Q"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
        "plt.rcParams['font.family'] = \"serif\"\n",
        "df = sns.load_dataset(\"flights\")\n",
        "df_new = df.rename(columns = {'year':'num of flows','month':'layer depth','passengers':'BPA'})\n",
        "df_new = df_new.drop(range(12,144))\n",
        "df_new['num of flows'] = pd.DataFrame([1,2,3,4,1,2,3,4,1,2,3,4])\n",
        "df_new['layer depth'] = pd.DataFrame([1,1,1,1,2,2,2,2,4,4,4,4])\n",
        "df_new['BPA'] = pd.DataFrame([4.01,4.48,5.36,5.84,4.58,5.48,6.38,6.66,4.78,5.54,6.43,7.22])\n",
        "\n",
        "df = pd.pivot_table(data=df_new,\n",
        "                    index='num of flows',\n",
        "                    values='BPA',\n",
        "                    columns='layer depth')\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.set(font_scale=1.6)\n",
        "p = sns.heatmap(df, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
        "# qm9 , unseen val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i81sA94WaPVg"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
        "plt.rcParams['font.family'] = \"serif\"\n",
        "df = sns.load_dataset(\"flights\")\n",
        "df_new = df.rename(columns = {'year':'num of flows','month':'layer depth','passengers':'BPA'})\n",
        "df_new = df_new.drop(range(12,144))\n",
        "df_new['num of flows'] = pd.DataFrame([1,2,3,4,1,2,3,4,1,2,3,4])\n",
        "df_new['layer depth'] = pd.DataFrame([1,1,1,1,2,2,2,2,4,4,4,4])\n",
        "df_new['BPA'] = pd.DataFrame([3.97,4.53,5.32,5.82,4.61,5.62,6.06,6.62,4.70,5.59,6.26,6.86])\n",
        "\n",
        "df = pd.pivot_table(data=df_new,\n",
        "                    index='num of flows',\n",
        "                    values='BPA',\n",
        "                    columns='layer depth')\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.set(font_scale=1.6)\n",
        "p = sns.heatmap(df, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
        "# qm9 , whole\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNIgCk3tgroY"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
        "plt.rcParams['font.family'] = \"serif\"\n",
        "df = sns.load_dataset(\"flights\")\n",
        "df_new = df.rename(columns = {'year':'num of flows','month':'layer depth','passengers':'BPA'})\n",
        "df_new = df_new.drop(range(12,144))\n",
        "df_new['num of flows'] = pd.DataFrame([1,2,3,1,2,3,1,2,3])\n",
        "df_new['layer depth'] = pd.DataFrame([1,1,1,2,2,2,4,4,4])\n",
        "df_new['BPA'] = pd.DataFrame([4.46,6.09,8.24,5.73,8.81, 12.34, 6.16, 9.80, 13.52])\n",
        "\n",
        "df = pd.pivot_table(data=df_new,\n",
        "                    index='num of flows',\n",
        "                    values='BPA',\n",
        "                    columns='layer depth')\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.set(font_scale=1.6)\n",
        "p = sns.heatmap(df, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
        "# zinc , some\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inPs-P-FhYGI"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
        "plt.rcParams['font.family'] = \"serif\"\n",
        "df = sns.load_dataset(\"flights\")\n",
        "df_new = df.rename(columns = {'year':'num of flows','month':'layer depth','passengers':'BPA'})\n",
        "df_new = df_new.drop(range(12,144))\n",
        "df_new['num of flows'] = pd.DataFrame([1,2,3,1,2,3,1,2,3])\n",
        "df_new['layer depth'] = pd.DataFrame([1,1,1,2,2,2,4,4,4])\n",
        "df_new['BPA'] = pd.DataFrame([9.97,17.47,25.65,13.43,25.49,37.67,20.84,24.04,38.30])\n",
        "\n",
        "df = pd.pivot_table(data=df_new,\n",
        "                    index='num of flows',\n",
        "                    values='BPA',\n",
        "                    columns='layer depth')\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.set(font_scale=1.6)\n",
        "p = sns.heatmap(df, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
        "# zinc to QM9\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2dEOe_VhrCI"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
        "plt.rcParams['font.family'] = \"serif\"\n",
        "df = sns.load_dataset(\"flights\")\n",
        "df_new = df.rename(columns = {'year':'num of flows','month':'layer depth','passengers':'BPA'})\n",
        "df_new = df_new.drop(range(12,144))\n",
        "df_new['num of flows'] = pd.DataFrame([1,2,3,1,2,3,1,2,3])\n",
        "df_new['layer depth'] = pd.DataFrame([1,1,1,2,2,2,4,4,4])\n",
        "df_new['BPA'] = pd.DataFrame([7.07,7.82,8.97,8.28,8.43,9.95,12.42,12.55,12.83])\n",
        "\n",
        "df = pd.pivot_table(data=df_new,\n",
        "                    index='num of flows',\n",
        "                    values='BPA',\n",
        "                    columns='layer depth')\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.set(font_scale=1.6)\n",
        "p = sns.heatmap(df, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
        "# QM9 to zinc\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy6EA1y0OVOa"
      },
      "source": [
        "! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "!conda install -c rdkit rdkit -y\n",
        "\n",
        "!pip install orderedset  \n",
        "!pip install tabulate  \n",
        "!pip install networkx  \n",
        "!pip install scipy  \n",
        "!pip install seaborn  \n",
        "\n",
        "!pip install cairosvg\n",
        "!pip install tqdm\n",
        "!pip install torch\n",
        "!pip install ipykernel\n",
        "!pip install torchvision\n",
        "\n",
        "%cd /content/drive/MyDrive/Machine_Learning/moflow-master/data\n",
        "!python data_preprocess.py --data_name qm9\n",
        "\n",
        "%cd /content/drive/MyDrive/Machine_Learning/moflow-master/data\n",
        "import transform_qm9\n",
        "data_file = 'qm9_relgcn_kekulized_ggnp.npz'\n",
        "transform_fn = transform_qm9.transform_fn\n",
        "atomic_num_list = [6, 7, 8, 9, 0]\n",
        "b_n_type = 4\n",
        "b_n_squeeze = 3\n",
        "a_n_node = 9\n",
        "a_n_type = len(atomic_num_list)  # 5\n",
        "valid_idx = transform_qm9.get_val_ids() \n",
        "print(valid_idx)\n",
        "\n",
        "from data_loader import NumpyTupleDataset\n",
        "dataset = NumpyTupleDataset.load('/content/drive/MyDrive/Machine_Learning/moflow-master/data/qm9_relgcn_kekulized_ggnp.npz', transform=transform_fn) \n",
        "\n",
        "import torch\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
        "                                                   shuffle=False, num_workers=0)\n",
        "\n",
        "atoms = []\n",
        "adj = []\n",
        "for i,batch in enumerate(dataloader):\n",
        "  atoms.append(batch[0])\n",
        "  adj.append(batch[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdmhoPZGfmHp"
      },
      "source": [
        "!pip install git+https://github.com/j-towns/craystack.git\n",
        "\n",
        "# Install multiset-compression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rEhMRGegS1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46e2490-db3c-4c58-81db-1e4e467d0621"
      },
      "source": [
        "%cd /content/drive/MyDrive/Machine_Learning/multiset-compression-master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Machine_Learning/multiset-compression-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMYivDFUmhfF"
      },
      "source": [
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilHDIJSbnNU3"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Graph stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTQd8j76s9Bq"
      },
      "source": [
        "atoms = []\n",
        "adj = []\n",
        "for i,batch in enumerate(dataloader):\n",
        "  atoms.append(batch[0])\n",
        "  adj.append(batch[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6snOh2H2pch0"
      },
      "source": [
        "import numpy as np\n",
        "symbols_count = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "for i in range(len(atoms)):\n",
        "    for atom in np.where(atoms[i]==1)[2]:\n",
        "        symbols_count[atom] += 1\n",
        "print('total_QM9',symbols_count) #the fifth type of atom is nothing\n",
        "\n",
        "list_of_multisets = [] #find a way to save bonds for each multiset\n",
        "for bonds in adj:\n",
        "  symbols = []\n",
        "  bonds = bonds[:,0,:,:] + bonds[:,1,:,:] * 2 + bonds[:,2,:,:] * 3 # collapsing 9 by 9 by 4 to simply 9 by 9\n",
        "  bonds = np.squeeze(bonds)\n",
        "  bonds = np.triu(bonds) # only need upper triangular half\n",
        "  for row_num in range(9):\n",
        "    row = bonds[row_num]\n",
        "    for ind_num in range(9):\n",
        "      ind = row[ind_num]\n",
        "      if ind:\n",
        "        symbols.append([row_num, ind_num, int(ind)+8]) #the bond type must have its own symbol, different to 0,....,8\n",
        "        symbols_count[row_num] += 1\n",
        "        symbols_count[ind_num] += 1\n",
        "        symbols_count[int(ind)+8] += 1\n",
        "  list_of_multisets.append(symbols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK8fJKJvuZ_c"
      },
      "source": [
        "print(symbols_count)\n",
        "print(symbols_count/ (sum(symbols_count)*np.ones(12))  )\n",
        "\n",
        "frequencies = symbols_count/ (sum(symbols_count)*np.ones(12)) \n",
        "print(frequencies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWfuLlFRX8hY"
      },
      "source": [
        "atom_vectors = []\n",
        "for i in range(len(atoms)):\n",
        "    atom_vectors.append(np.where(atoms[i]==1)[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JdUqM1En_c6"
      },
      "source": [
        "%cd /content/drive/MyDrive/Machine_Learning/multiset-compression-master\n",
        "from utils import calculate_state_bits, cache\n",
        "from multiset_codec import codecs, msbst, rans\n",
        "from multiset_codec.msbst import *\n",
        "from scipy.stats import multinomial, dirichlet\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import craystack as cs\n",
        "\n",
        "# multisets = [[1,2,9],[2,3,9],[3,4,9],[4,5,9],[5,6,9],[5,7,9],[5,8,9],[0,1,11]]\n",
        "\n",
        "_probs = frequencies # we need an alphabet!!!!!!\n",
        "swor_codec = codecs.SamplingWithoutReplacement()\n",
        "# symbol_codec = codecs.Uniform(12)\n",
        "symbol_codec = codecs.Categorical(_probs, prec=27)\n",
        "multiset_codec = codecs.Multiset(symbol_codec)\n",
        "qm9_num = 1 #meaningless variable which is printed\n",
        "new_ans_state = rans.base_message(shape=(1,))\n",
        "for j in range(len(list_of_multisets)):\n",
        "    multisets = list_of_multisets[j]\n",
        "    print(qm9_num)\n",
        "    qm9_num += 1\n",
        "    multiset_outer = build_multiset([build_multiset(bond_triple) for bond_triple in multisets])\n",
        "    original_multiset = multiset_outer\n",
        "    # print('original_multiset', original_multiset)\n",
        "    ans_state = new_ans_state\n",
        "    \n",
        "    #encode atom types as a sequence\n",
        "    (ans_state,) = codecs.Sequence(symbol_codec).encode(ans_state,atom_vectors[j])\n",
        "    #encode multiset\n",
        "    while multiset_outer:\n",
        "        # 1) Sample, without replacement, an inner multiset using ANS decode\n",
        "        ans_state, multiset_inner, multiset_outer = swor_codec.decode(ans_state, multiset_outer)\n",
        "        # print('\\n multiset_inner',type(np.array(to_sequence(multiset_inner))), multiset_inner)\n",
        "        multiset_inner = build_multiset((np.array(to_sequence(multiset_inner))))\n",
        "        # print('\\n swor decode ans_state head', ans_state[0], 'tail', ans_state[1])\n",
        "        (ans_state,) = multiset_codec.encode(ans_state, multiset_inner)\n",
        "        # print('\\n multiset.encode', ans_state[0], 'tail', ans_state[1])\n",
        "        compressed_length_multiset = calculate_state_bits(ans_state)\n",
        "\n",
        "    print(compressed_length_multiset/qm9_num)\n",
        "    new_ans_state = ans_state\n",
        "\n",
        "\n",
        "    '''#  This stuff all works!!!!!! I commented out the decoding part to save run time\n",
        "    multiset_decoded = ()\n",
        "    for _ in range(len(multisets)):\n",
        "        (ans_state, multiset_inner_decoded) = multiset_codec.decode(ans_state, multiset_size=3)\n",
        "        # print('\\n multiset_inner', to_sequence(multiset_inner_decoded))\n",
        "        # print('\\n multiset.decode ans_state head', ans_state[0], 'tail', ans_state[1])\n",
        "        multiset_inner_decoded = build_multiset(sorted(to_sequence(multiset_inner_decoded)))\n",
        "        (ans_state, multiset_decoded) = swor_codec.encode(ans_state, multiset_inner_decoded, multiset_decoded)\n",
        "        # print('\\n multiset_decoded', multiset_decoded)\n",
        "        # print('\\n swor encode ans_state head', ans_state[0], 'tail', ans_state[1])\n",
        "\n",
        "    assert check_multiset_equality(original_multiset, multiset_decoded)\n",
        "    ans_state, recon_atom_vec= codecs.Sequence(symbol_codec).decode(ans_state, 9)\n",
        "    np.testing.assert_array_equal(recon_atom_vec,atom_vectors[j])'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQcV9qH63e5T"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6-v9UDn3m58",
        "outputId": "14e631ce-47d7-42ba-807d-cd07090af35c"
      },
      "source": [
        "atoms = []\n",
        "for i,batch in enumerate(dataloader):\n",
        "  atoms.append(batch[0])\n",
        "\n",
        "import numpy as np\n",
        "symbols_count = [0,0,0,0,0]\n",
        "for i in range(len(atoms)):\n",
        "    for atom in np.where(atoms[i]==1)[2]:\n",
        "        symbols_count[atom] += 1\n",
        "print('total_QM9',symbols_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_QM9 [846556, 139764, 187997, 3314, 27334]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy3vGsxy4iQC",
        "outputId": "4d920d74-233c-46cc-b813-accee7334978"
      },
      "source": [
        "print(symbols_count)\n",
        "print(symbols_count/ (sum(symbols_count)*np.ones(5))  )\n",
        "\n",
        "frequencies = symbols_count/ (sum(symbols_count)*np.ones(5)) \n",
        "print(frequencies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[846556, 139764, 187997, 3314, 27334]\n",
            "[0.70255651 0.11599009 0.15601864 0.00275029 0.02268448]\n",
            "[0.70255651 0.11599009 0.15601864 0.00275029 0.02268448]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvS1zPqQ5BiY"
      },
      "source": [
        "atom_vectors = []\n",
        "for i in range(len(atoms)):\n",
        "    atom_vectors.append(np.where(atoms[i]==1)[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JaZ2_ZfWnYP"
      },
      "source": [
        "%cd /content/drive/MyDrive/Machine_Learning/multiset-compression-master\n",
        "from utils import calculate_state_bits, cache\n",
        "from multiset_codec import codecs, msbst, rans\n",
        "from multiset_codec.msbst import *\n",
        "from scipy.stats import multinomial, dirichlet\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import craystack as cs\n",
        "\n",
        "# multisets = [[1,2,9],[2,3,9],[3,4,9],[4,5,9],[5,6,9],[5,7,9],[5,8,9],[0,1,11]]\n",
        "\n",
        "_probs = frequencies # we need an alphabet!!!!!!\n",
        "swor_codec = codecs.SamplingWithoutReplacement()\n",
        "# symbol_codec = codecs.Uniform(12)\n",
        "symbol_codec = codecs.Categorical(_probs, prec=27)\n",
        "multiset_codec = codecs.Multiset(symbol_codec)\n",
        "qm9_num = 1 #meaningless variable which is printed\n",
        "new_ans_state = rans.base_message(shape=(1,))\n",
        "for j in range(len(atoms)):\n",
        "    # multisets = list_of_multisets[j]\n",
        "    print(qm9_num)\n",
        "    qm9_num += 1\n",
        "    # multiset_outer = build_multiset([build_multiset(bond_triple) for bond_triple in multisets])\n",
        "    # original_multiset = multiset_outer\n",
        "    # # print('original_multiset', original_multiset)\n",
        "    ans_state = new_ans_state\n",
        "    \n",
        "    #encode atom types as a sequence\n",
        "    (ans_state,) = codecs.Sequence(symbol_codec).encode(ans_state,atom_vectors[j])\n",
        "    # #encode multiset\n",
        "    # while multiset_outer:\n",
        "    #     # 1) Sample, without replacement, an inner multiset using ANS decode\n",
        "    #     ans_state, multiset_inner, multiset_outer = swor_codec.decode(ans_state, multiset_outer)\n",
        "    #     # print('\\n multiset_inner',type(np.array(to_sequence(multiset_inner))), multiset_inner)\n",
        "    #     multiset_inner = build_multiset((np.array(to_sequence(multiset_inner))))\n",
        "    #     # print('\\n swor decode ans_state head', ans_state[0], 'tail', ans_state[1])\n",
        "    #     (ans_state,) = multiset_codec.encode(ans_state, multiset_inner)\n",
        "    #     # print('\\n multiset.encode', ans_state[0], 'tail', ans_state[1])\n",
        "    compressed_length_multiset = calculate_state_bits(ans_state)\n",
        "\n",
        "    print(compressed_length_multiset/qm9_num)\n",
        "    new_ans_state = ans_state\n",
        "\n",
        "\n",
        "    '''#  This stuff all works!!!!!! I commented out the decoding part to save run time\n",
        "    multiset_decoded = ()\n",
        "    for _ in range(len(multisets)):\n",
        "        (ans_state, multiset_inner_decoded) = multiset_codec.decode(ans_state, multiset_size=3)\n",
        "        # print('\\n multiset_inner', to_sequence(multiset_inner_decoded))\n",
        "        # print('\\n multiset.decode ans_state head', ans_state[0], 'tail', ans_state[1])\n",
        "        multiset_inner_decoded = build_multiset(sorted(to_sequence(multiset_inner_decoded)))\n",
        "        (ans_state, multiset_decoded) = swor_codec.encode(ans_state, multiset_inner_decoded, multiset_decoded)\n",
        "        # print('\\n multiset_decoded', multiset_decoded)\n",
        "        # print('\\n swor encode ans_state head', ans_state[0], 'tail', ans_state[1])\n",
        "\n",
        "    assert check_multiset_equality(original_multiset, multiset_decoded)\n",
        "    ans_state, recon_atom_vec= codecs.Sequence(symbol_codec).decode(ans_state, 9)\n",
        "    np.testing.assert_array_equal(recon_atom_vec,atom_vectors[j])'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}